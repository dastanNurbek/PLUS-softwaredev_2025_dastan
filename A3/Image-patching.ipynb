{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab66570-f90e-4ba0-be7c-bb1094f75071",
   "metadata": {},
   "source": [
    "# Image Patch Splitting for CNN Models\n",
    "\n",
    "This notebook demonstrates various techniques for splitting images into patches, which is useful for CNN training, data augmentation, and handling large images. Using [open-cv](https://pypi.org/project/opencv-python/), [scikit-learn](https://pypi.org/project/scikit-learn/), [PyTorch](https://pytorch.org/), [torchvision](https://docs.pytorch.org/vision/stable/index.html)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Setup and Imports](#2.-Setup-and-Imports)\n",
    "3. [Basic Image Patching](#3.-Basic-Image-Patching)\n",
    "4. [Advanced Patching Techniques](#4.-Advanced-Patching-Techniques)\n",
    "5. [Patch Reconstruction](#5.-Patch-Reconstruction)\n",
    "6. [CNN Integration Examples](#6.-CNN-Integration-Examples)\n",
    "7. [Performance Optimization](#7.-Performance-Optimization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada828e-d27d-48fa-83c7-8e425a83b9df",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Image patching is a technique where we divide a large image into smaller, overlapping or non-overlapping patches. This is useful for:\n",
    "- Training CNNs on high-resolution images\n",
    "- Data augmentation\n",
    "- Memory efficiency\n",
    "- Handling images larger than model input size\n",
    "- Creating sliding window predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dcf2f1-234b-4686-8221-9b463ef7d462",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9b1fe-c35c-4b46-974a-ea0a889ea333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.feature_extraction import image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4cbc31-80ae-4c52-8a39-61b1567e0826",
   "metadata": {},
   "source": [
    "## 3. Basic Image Patching\n",
    "\n",
    "### 3.1 Load and Prepare Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356eb9b-0779-4bed-98eb-2d3193009af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample image or load your own\n",
    "def create_sample_image(height=512, width=512):\n",
    "    \"\"\"Create a sample image with patterns for demonstration\"\"\"\n",
    "    image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Add some patterns\n",
    "    for i in range(0, height, 64):\n",
    "        for j in range(0, width, 64):\n",
    "            color = ((i//64 + j//64) % 3) * 80 + 50\n",
    "            image[i:i+64, j:j+64] = [color, (color + 50) % 255, (color + 100) % 255]\n",
    "    \n",
    "    # Add some noise for texture\n",
    "    noise = np.random.randint(0, 30, (height, width, 3))\n",
    "    image = np.clip(image + noise, 0, 255)\n",
    "    \n",
    "    return image.astype(np.uint8)\n",
    "\n",
    "# Create sample image\n",
    "sample_image = create_sample_image(512, 512)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Original Image (512x512)')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original image shape: {sample_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca1817-b820-47ae-b10f-89100d50bf0b",
   "metadata": {},
   "source": [
    "### 3.2 Simple Non-Overlapping Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd6d17-7dea-43f1-9b66-86df9120a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches_simple(image, patch_size):\n",
    "    \"\"\"\n",
    "    Extract non-overlapping patches from an image\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (H, W, C) or (H, W)\n",
    "        patch_size: Size of each patch (height, width)\n",
    "    \n",
    "    Returns:\n",
    "        patches: Array of patches\n",
    "        positions: Positions of each patch in original image\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 2:\n",
    "        h, w = image.shape\n",
    "        c = 1\n",
    "        image = image.reshape(h, w, 1)\n",
    "    else:\n",
    "        h, w, c = image.shape\n",
    "    \n",
    "    patch_h, patch_w = patch_size\n",
    "    \n",
    "    # Calculate number of patches\n",
    "    n_patches_h = h // patch_h\n",
    "    n_patches_w = w // patch_w\n",
    "    \n",
    "    patches = []\n",
    "    positions = []\n",
    "    \n",
    "    for i in range(n_patches_h):\n",
    "        for j in range(n_patches_w):\n",
    "            # Extract patch\n",
    "            start_h = i * patch_h\n",
    "            end_h = start_h + patch_h\n",
    "            start_w = j * patch_w\n",
    "            end_w = start_w + patch_w\n",
    "            \n",
    "            patch = image[start_h:end_h, start_w:end_w]\n",
    "            patches.append(patch)\n",
    "            positions.append((start_h, start_w))\n",
    "    \n",
    "    return np.array(patches), positions\n",
    "\n",
    "# Extract 64x64 patches\n",
    "patches, positions = extract_patches_simple(sample_image, (64, 64))\n",
    "\n",
    "print(f\"Number of patches: {len(patches)}\")\n",
    "print(f\"Patch shape: {patches[0].shape}\")\n",
    "\n",
    "# Visualize first 16 patches\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(patches):\n",
    "        ax.imshow(patches[i])\n",
    "        ax.set_title(f'Patch {i+1}\\nPos: {positions[i]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc3517-0066-45b8-a7a7-d4f2d1c00d4b",
   "metadata": {},
   "source": [
    "### 3.3 Overlapping Patches with Stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b9ba5-6001-4b64-82a2-91e4d40eb655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches_stride(image, patch_size, stride):\n",
    "    \"\"\"\n",
    "    Extract overlapping patches using stride\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (H, W, C)\n",
    "        patch_size: Size of each patch (height, width)\n",
    "        stride: Stride for patch extraction (stride_h, stride_w)\n",
    "    \n",
    "    Returns:\n",
    "        patches: Array of patches\n",
    "        positions: Positions of each patch\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 2:\n",
    "        h, w = image.shape\n",
    "        image = image.reshape(h, w, 1)\n",
    "    else:\n",
    "        h, w, c = image.shape\n",
    "    \n",
    "    patch_h, patch_w = patch_size\n",
    "    stride_h, stride_w = stride\n",
    "    \n",
    "    patches = []\n",
    "    positions = []\n",
    "    \n",
    "    # Calculate possible positions\n",
    "    for i in range(0, h - patch_h + 1, stride_h):\n",
    "        for j in range(0, w - patch_w + 1, stride_w):\n",
    "            patch = image[i:i+patch_h, j:j+patch_w]\n",
    "            patches.append(patch)\n",
    "            positions.append((i, j))\n",
    "    \n",
    "    return np.array(patches), positions\n",
    "\n",
    "# Extract overlapping patches with stride=32\n",
    "overlapping_patches, overlap_positions = extract_patches_stride(\n",
    "    sample_image, (64, 64), (32, 32)\n",
    ")\n",
    "\n",
    "print(f\"Number of overlapping patches: {len(overlapping_patches)}\")\n",
    "print(f\"Overlap ratio: {len(overlapping_patches) / len(patches):.2f}x more patches\")\n",
    "\n",
    "# Visualize overlap pattern\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Original Image')\n",
    "\n",
    "# Draw patch boundaries\n",
    "for pos in overlap_positions[:25]:  # Show first 25 patches\n",
    "    rect = plt.Rectangle(pos[::-1], 64, 64, fill=False, edgecolor='red', alpha=0.5)\n",
    "    plt.gca().add_patch(rect)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show some overlapping patches\n",
    "fig_patches = overlapping_patches[:16]\n",
    "# Convert to float and normalize to [0, 1] range\n",
    "patches_tensor = torch.from_numpy(fig_patches).float() / 255.0\n",
    "grid_img = make_grid(patches_tensor.permute(0, 3, 1, 2), nrow=4, normalize=False)\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.title('Sample Overlapping Patches')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ff299-8649-408d-ab48-30f614ffb1a6",
   "metadata": {},
   "source": [
    "## 4. Advanced Patching Techniques\n",
    "\n",
    "### 4.1 Using sklearn's extract_patches_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a5668-5748-45ff-8671-876f330c4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import image as sk_image\n",
    "\n",
    "def extract_patches_sklearn(image, patch_size, max_patches=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Extract patches using sklearn's extract_patches_2d\n",
    "    \n",
    "    Args:\n",
    "        image: Input image\n",
    "        patch_size: (height, width) of patches\n",
    "        max_patches: Maximum number of patches to extract (None for all)\n",
    "        random_state: Random seed for patch selection\n",
    "    \n",
    "    Returns:\n",
    "        patches: Extracted patches\n",
    "    \"\"\"\n",
    "    # sklearn works with grayscale or single channel, so we'll process each channel\n",
    "    if len(image.shape) == 3:\n",
    "        patches_list = []\n",
    "        for channel in range(image.shape[2]):\n",
    "            channel_patches = sk_image.extract_patches_2d(\n",
    "                image[:, :, channel], \n",
    "                patch_size, \n",
    "                max_patches=max_patches,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            patches_list.append(channel_patches)\n",
    "        \n",
    "        # Combine channels\n",
    "        patches = np.stack(patches_list, axis=-1)\n",
    "    else:\n",
    "        patches = sk_image.extract_patches_2d(\n",
    "            image, patch_size, max_patches=max_patches, random_state=random_state\n",
    "        )\n",
    "    \n",
    "    return patches\n",
    "\n",
    "# Extract random patches using sklearn\n",
    "sklearn_patches = extract_patches_sklearn(sample_image, (64, 64), max_patches=100)\n",
    "print(f\"Sklearn patches shape: {sklearn_patches.shape}\")\n",
    "\n",
    "# Visualize random patches\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(sklearn_patches):\n",
    "        ax.imshow(sklearn_patches[i])\n",
    "        ax.set_title(f'Random Patch {i+1}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f969d0-81f6-4289-b63a-ffc3a121a258",
   "metadata": {},
   "source": [
    "### 4.2 Patch Extraction with Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266c0e8-f557-4da1-aaca-b9bb43515608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches_with_padding(image, patch_size, stride, padding_mode='reflect'):\n",
    "    \"\"\"\n",
    "    Extract patches with padding to handle edge cases\n",
    "    \n",
    "    Args:\n",
    "        image: Input image\n",
    "        patch_size: (height, width) of patches\n",
    "        stride: (stride_h, stride_w)\n",
    "        padding_mode: Padding mode ('reflect', 'constant', 'edge', 'wrap')\n",
    "    \n",
    "    Returns:\n",
    "        patches: Extracted patches\n",
    "        positions: Original positions before padding\n",
    "    \"\"\"\n",
    "    patch_h, patch_w = patch_size\n",
    "    stride_h, stride_w = stride\n",
    "    \n",
    "    # Calculate required padding\n",
    "    h, w = image.shape[:2]\n",
    "    pad_h = (patch_h - h % stride_h) % stride_h\n",
    "    pad_w = (patch_w - w % stride_w) % stride_w\n",
    "    \n",
    "    # Pad image\n",
    "    if len(image.shape) == 3:\n",
    "        padded_image = np.pad(image, \n",
    "                             ((0, pad_h), (0, pad_w), (0, 0)), \n",
    "                             mode=padding_mode)\n",
    "    else:\n",
    "        padded_image = np.pad(image, \n",
    "                             ((0, pad_h), (0, pad_w)), \n",
    "                             mode=padding_mode)\n",
    "    \n",
    "    # Extract patches from padded image\n",
    "    patches, positions = extract_patches_stride(padded_image, patch_size, stride)\n",
    "    \n",
    "    return patches, positions, padded_image\n",
    "\n",
    "# Extract patches with padding\n",
    "padded_patches, padded_positions, padded_img = extract_patches_with_padding(\n",
    "    sample_image, (64, 64), (64, 64), padding_mode='reflect'\n",
    ")\n",
    "\n",
    "print(f\"Original image shape: {sample_image.shape}\")\n",
    "print(f\"Padded image shape: {padded_img.shape}\")\n",
    "print(f\"Number of patches with padding: {len(padded_patches)}\")\n",
    "\n",
    "# Visualize padding effect\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(padded_img)\n",
    "plt.title('Padded Image')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392bedd-5a4e-474c-8186-8a1ba998603d",
   "metadata": {},
   "source": [
    "## 5. Patch Reconstruction\n",
    "\n",
    "### 5.1 Reconstruct from Non-Overlapping Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71890c8e-eec8-42c1-b84f-dc79afecf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_patches(patches, positions, original_shape, patch_size):\n",
    "    \"\"\"\n",
    "    Reconstruct image from non-overlapping patches\n",
    "    \n",
    "    Args:\n",
    "        patches: Array of patches\n",
    "        positions: Positions of each patch\n",
    "        original_shape: Shape of original image\n",
    "        patch_size: Size of each patch\n",
    "    \n",
    "    Returns:\n",
    "        reconstructed_image: Reconstructed image\n",
    "    \"\"\"\n",
    "    if len(original_shape) == 3:\n",
    "        h, w, c = original_shape\n",
    "        reconstructed = np.zeros((h, w, c), dtype=patches.dtype)\n",
    "    else:\n",
    "        h, w = original_shape\n",
    "        reconstructed = np.zeros((h, w), dtype=patches.dtype)\n",
    "        c = 1\n",
    "    \n",
    "    patch_h, patch_w = patch_size\n",
    "    \n",
    "    for patch, (start_h, start_w) in zip(patches, positions):\n",
    "        end_h = min(start_h + patch_h, h)\n",
    "        end_w = min(start_w + patch_w, w)\n",
    "        \n",
    "        if len(original_shape) == 3:\n",
    "            reconstructed[start_h:end_h, start_w:end_w] = patch\n",
    "        else:\n",
    "            reconstructed[start_h:end_h, start_w:end_w] = patch.squeeze()\n",
    "    \n",
    "    return reconstructed\n",
    "\n",
    "# Reconstruct image\n",
    "reconstructed = reconstruct_from_patches(patches, positions, sample_image.shape, (64, 64))\n",
    "\n",
    "# Compare original and reconstructed\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reconstructed)\n",
    "plt.title('Reconstructed Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if reconstruction is perfect\n",
    "print(f\"Reconstruction error: {np.mean(np.abs(sample_image - reconstructed))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586876f-9fdf-4236-a2ad-e9f69f5568cb",
   "metadata": {},
   "source": [
    "### 5.2 Reconstruct from Overlapping Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6063185-2deb-4275-80e3-d5312838c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_overlapping_patches(patches, positions, original_shape, patch_size):\n",
    "    \"\"\"\n",
    "    Reconstruct image from overlapping patches using averaging\n",
    "    \n",
    "    Args:\n",
    "        patches: Array of patches\n",
    "        positions: Positions of each patch\n",
    "        original_shape: Shape of original image\n",
    "        patch_size: Size of each patch\n",
    "    \n",
    "    Returns:\n",
    "        reconstructed_image: Reconstructed image\n",
    "    \"\"\"\n",
    "    if len(original_shape) == 3:\n",
    "        h, w, c = original_shape\n",
    "        reconstructed = np.zeros((h, w, c), dtype=np.float64)\n",
    "        count_map = np.zeros((h, w, c), dtype=np.float64)\n",
    "    else:\n",
    "        h, w = original_shape\n",
    "        reconstructed = np.zeros((h, w), dtype=np.float64)\n",
    "        count_map = np.zeros((h, w), dtype=np.float64)\n",
    "    \n",
    "    patch_h, patch_w = patch_size\n",
    "    \n",
    "    for patch, (start_h, start_w) in zip(patches, positions):\n",
    "        end_h = min(start_h + patch_h, h)\n",
    "        end_w = min(start_w + patch_w, w)\n",
    "        \n",
    "        patch_actual = patch[:end_h-start_h, :end_w-start_w]\n",
    "        \n",
    "        if len(original_shape) == 3:\n",
    "            reconstructed[start_h:end_h, start_w:end_w] += patch_actual\n",
    "            count_map[start_h:end_h, start_w:end_w] += 1\n",
    "        else:\n",
    "            reconstructed[start_h:end_h, start_w:end_w] += patch_actual.squeeze()\n",
    "            count_map[start_h:end_h, start_w:end_w] += 1\n",
    "    \n",
    "    # Average overlapping regions\n",
    "    reconstructed = reconstructed / np.maximum(count_map, 1)\n",
    "    \n",
    "    return reconstructed.astype(np.uint8)\n",
    "\n",
    "# Reconstruct from overlapping patches\n",
    "overlap_reconstructed = reconstruct_from_overlapping_patches(\n",
    "    overlapping_patches, overlap_positions, sample_image.shape, (64, 64)\n",
    ")\n",
    "\n",
    "# Compare reconstructions\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Original')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(reconstructed)\n",
    "plt.title('Non-overlapping Reconstruction')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(overlap_reconstructed)\n",
    "plt.title('Overlapping Reconstruction')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Non-overlapping reconstruction error: {np.mean(np.abs(sample_image - reconstructed))}\")\n",
    "print(f\"Overlapping reconstruction error: {np.mean(np.abs(sample_image - overlap_reconstructed))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0632a3a-e7cd-4b4b-935e-986036d8fb1e",
   "metadata": {},
   "source": [
    "## 6. CNN Integration Examples\n",
    "\n",
    "### 6.1 PyTorch Dataset for Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58971637-77dc-4481-bdff-77bf8e724608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    \"\"\"Dataset class for image patches\"\"\"\n",
    "    \n",
    "    def __init__(self, images, patch_size, stride=None, transform=None, max_patches_per_image=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: List of images or single image\n",
    "            patch_size: (height, width) of patches\n",
    "            stride: Stride for patch extraction (default: patch_size for non-overlapping)\n",
    "            transform: Optional transform to be applied to patches\n",
    "            max_patches_per_image: Maximum patches per image (for memory management)\n",
    "        \"\"\"\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride if stride is not None else patch_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Extract all patches\n",
    "        self.patches = []\n",
    "        self.labels = []  # You can add labels here if needed\n",
    "        \n",
    "        if not isinstance(images, list):\n",
    "            images = [images]\n",
    "        \n",
    "        for img_idx, image in enumerate(images):\n",
    "            patches, _ = extract_patches_stride(image, patch_size, self.stride)\n",
    "            \n",
    "            # Limit patches per image if specified\n",
    "            if max_patches_per_image and len(patches) > max_patches_per_image:\n",
    "                indices = np.random.choice(len(patches), max_patches_per_image, replace=False)\n",
    "                patches = patches[indices]\n",
    "            \n",
    "            self.patches.extend(patches)\n",
    "            self.labels.extend([img_idx] * len(patches))  # Image index as label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patch = self.patches[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert to PIL Image for transforms\n",
    "        if self.transform:\n",
    "            patch = Image.fromarray(patch)\n",
    "            patch = self.transform(patch)\n",
    "        else:\n",
    "            # Convert to tensor\n",
    "            patch = torch.from_numpy(patch).float().permute(2, 0, 1) / 255.0\n",
    "        \n",
    "        return patch, label\n",
    "\n",
    "# Create dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = PatchDataset([sample_image], patch_size=(64, 64), stride=(32, 32), \n",
    "                      transform=transform, max_patches_per_image=100)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Visualize a batch\n",
    "batch_patches, batch_labels = next(iter(dataloader))\n",
    "print(f\"Batch shape: {batch_patches.shape}\")\n",
    "\n",
    "# Denormalize for visualization\n",
    "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return torch.clamp(tensor, 0, 1)\n",
    "\n",
    "grid = make_grid(denormalize(batch_patches.clone()), nrow=4)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(grid.permute(1, 2, 0))\n",
    "plt.title('Batch of Patches')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5745d62f-7761-40d9-a00b-44f32517099e",
   "metadata": {},
   "source": [
    "### 6.2 Simple CNN for Patch Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94494877-0f6e-4eb8-a9c1-a85eb0e82554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchCNN(nn.Module):\n",
    "    \"\"\"Simple CNN for patch classification\"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels=3, num_classes=10, patch_size=64):\n",
    "        super(PatchCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        # Calculate the size after conv layers\n",
    "        conv_output_size = patch_size // 8  # After 3 MaxPool2d layers\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = PatchCNN(input_channels=3, num_classes=5, patch_size=64)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(4, 3, 64, 64)\n",
    "output = model(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b84fc3-da17-4e2c-8577-4e8aaffbbcaf",
   "metadata": {},
   "source": [
    "## 7. Performance Optimization\n",
    "\n",
    "### 7.1 Vectorized Patch Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ffa32-3777-4b37-8b41-7b9491aa5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches_vectorized(image, patch_size, stride):\n",
    "    \"\"\"\n",
    "    Vectorized patch extraction using numpy stride tricks\n",
    "    Much faster for large images\n",
    "    \"\"\"\n",
    "    from numpy.lib.stride_tricks import sliding_window_view\n",
    "    \n",
    "    if len(image.shape) == 2:\n",
    "        h, w = image.shape\n",
    "        image = image.reshape(h, w, 1)\n",
    "    \n",
    "    h, w, c = image.shape\n",
    "    patch_h, patch_w = patch_size\n",
    "    stride_h, stride_w = stride\n",
    "    \n",
    "    # Use sliding window view for efficient patch extraction\n",
    "    patches = sliding_window_view(image, (patch_h, patch_w, c), axis=(0, 1, 2))\n",
    "    \n",
    "    # Subsample according to stride\n",
    "    patches = patches[::stride_h, ::stride_w, 0]  # Remove singleton dimension\n",
    "    \n",
    "    # Reshape to (n_patches, patch_h, patch_w, c)\n",
    "    n_patches_h, n_patches_w = patches.shape[:2]\n",
    "    patches = patches.reshape(-1, patch_h, patch_w, c)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "# Compare performance\n",
    "import time\n",
    "\n",
    "# Time regular method\n",
    "start_time = time.time()\n",
    "regular_patches, _ = extract_patches_stride(sample_image, (64, 64), (16, 16))\n",
    "regular_time = time.time() - start_time\n",
    "\n",
    "# Time vectorized method\n",
    "start_time = time.time()\n",
    "vectorized_patches = extract_patches_vectorized(sample_image, (64, 64), (16, 16))\n",
    "vectorized_time = time.time() - start_time\n",
    "\n",
    "print(f\"Regular method: {regular_time:.4f} seconds, {len(regular_patches)} patches\")\n",
    "print(f\"Vectorized method: {vectorized_time:.4f} seconds, {len(vectorized_patches)} patches\")\n",
    "print(f\"Speedup: {regular_time/vectorized_time:.2f}x\")\n",
    "print(f\"Results identical: {np.allclose(regular_patches, vectorized_patches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21410c22-d29a-454a-9bdd-d6a3cfa6452b",
   "metadata": {},
   "source": [
    "### 7.2 Memory-Efficient Patch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ba6e3-6d85-42bf-9474-8e4c652e26c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientPatchProcessor:\n",
    "    \"\"\"Memory-efficient patch processor for large images\"\"\"\n",
    "    \n",
    "    def __init__(self, patch_size, stride, batch_size=32):\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def process_image_in_batches(self, image, process_fn):\n",
    "        \"\"\"\n",
    "        Process image patches in batches to save memory\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "            process_fn: Function to apply to each batch of patches\n",
    "        \n",
    "        Returns:\n",
    "            results: List of processing results\n",
    "        \"\"\"\n",
    "        patches, positions = extract_patches_stride(image, self.patch_size, self.stride)\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(patches), self.batch_size):\n",
    "            batch_patches = patches[i:i+self.batch_size]\n",
    "            batch_positions = positions[i:i+self.batch_size]\n",
    "            \n",
    "            # Convert to tensor\n",
    "            batch_tensor = torch.from_numpy(batch_patches).float().permute(0, 3, 1, 2) / 255.0\n",
    "            \n",
    "            # Process batch\n",
    "            with torch.no_grad():\n",
    "                batch_result = process_fn(batch_tensor)\n",
    "            \n",
    "            results.extend(batch_result.cpu().numpy())\n",
    "        \n",
    "        return results, positions\n",
    "\n",
    "# Example processing function (feature extraction)\n",
    "def extract_features(patch_batch):\n",
    "    \"\"\"Dummy feature extraction function\"\"\"\n",
    "    # Simple average pooling as feature\n",
    "    # Use reshape instead of view, or make tensor contiguous first\n",
    "    return torch.mean(patch_batch.reshape(patch_batch.size(0), -1), dim=1)\n",
    "\n",
    "# Process image in batches\n",
    "processor = MemoryEfficientPatchProcessor((64, 64), (32, 32), batch_size=16)\n",
    "features, patch_positions = processor.process_image_in_batches(sample_image, extract_features)\n",
    "\n",
    "print(f\"Extracted {len(features)} features from patches\")\n",
    "print(f\"Feature shape: {np.array(features).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65072583-b099-49e3-8887-441854cc2d2a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook covered various techniques for splitting images into patches for CNN models:\n",
    "\n",
    "1. **Basic patching**: Non-overlapping and overlapping patches\n",
    "2. **Advanced techniques**: Using sklearn, padding, and vectorized operations\n",
    "3. **Reconstruction**: Rebuilding images from patches\n",
    "4. **CNN integration**: PyTorch datasets and model examples\n",
    "5. **Performance optimization**: Vectorized operations and memory-efficient processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
